"""
LLM Agent –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–∞–Ω–¥ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å Ollama
"""
import json
import asyncio
import aiohttp
from typing import Dict, List, Optional, Any
from PIL import Image
import io
import base64

from ..utils.config import Config


class LLMAgent:
    """–ê–≥–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ Ollama"""
    
    def __init__(self, config: Config):
        self.config = config
        self.base_url = config.llm.base_url
        self.model = config.llm.model
        self.vision_model = config.llm.vision_model
        self.session = None
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ HTTP —Å–µ—Å—Å–∏–∏"""
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=180)  # 3 –º–∏–Ω—É—Ç—ã –¥–ª—è Steam Deck —Å —É—á–µ—Ç–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            )
        return self.session
    
    async def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LLM —Å–µ—Ä–≤–∏—Å–∞"""
        provider = self.config.llm.provider.lower()
        
        if provider == "ollama":
            return await self._check_ollama_availability()
        elif provider in ["openai", "deepseek", "anthropic"]:
            return await self._check_external_api_availability()
        else:
            print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider}")
            return False
    
    async def _check_ollama_availability(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama"""
        try:
            session = await self._get_session()
            async with session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    models_data = await response.json()
                    available_models = [model['name'] for model in models_data.get('models', [])]
                    print(f"ü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ Ollama –º–æ–¥–µ–ª–∏: {available_models}")
                    
                    if self.model not in available_models:
                        print(f"‚ùå –ú–æ–¥–µ–ª—å {self.model} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
                        print(f"üí° –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å: ollama pull {self.model}")
                        return False
                    
                    return True
                return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Ollama: {e}")
            print(f"üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–Ω–µ—à–Ω–∏–π API –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã")
            return False
    
    async def _check_external_api_availability(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤–Ω–µ—à–Ω–µ–≥–æ API"""
        try:
            api_key = getattr(self.config.llm, 'api_key', None)
            if not api_key:
                print(f"‚ùå API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è {self.config.llm.provider}")
                print(f"üí° –î–æ–±–∞–≤—å—Ç–µ api_key –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")
                return False
            
            print(f"‚úÖ {self.config.llm.provider} API –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            print(f"ü§ñ –ú–æ–¥–µ–ª—å: {self.model}")
            print(f"üëÅÔ∏è Vision –º–æ–¥–µ–ª—å: {self.vision_model}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {self.config.llm.provider} API: {e}")
            return False
    
    async def test_model(self) -> bool:
        """–ü—Ä–æ—Å—Ç–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∫–æ—Ä–æ—Ç–∫–∏–º –∑–∞–ø—Ä–æ—Å–æ–º"""
        try:
            print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å {self.model} –ø—Ä–æ—Å—Ç—ã–º –∑–∞–ø—Ä–æ—Å–æ–º...")
            
            session = await self._get_session()
            test_payload = {
                "model": self.model,
                "prompt": "–ü—Ä–∏–≤–µ—Ç! –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: —Ä–∞–±–æ—Ç–∞–µ—Ç?",
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 10  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
                }
            }
            
            import time
            start_time = time.time()
            
            async with session.post(f"{self.base_url}/api/generate", json=test_payload) as response:
                elapsed = time.time() - start_time
                
                if response.status == 200:
                    result = await response.json()
                    response_text = result.get('response', '').strip()
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ {elapsed:.1f}—Å: '{response_text}'")
                    return True
                else:
                    error_text = await response.text()
                    print(f"‚ùå –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ –Ω–µ—É–¥–∞—á–µ–Ω {response.status}: {error_text}")
                    return False
                    
        except Exception as e:
            error_type = type(e).__name__
            if "Timeout" in error_type:
                print(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ (>180s)")
                print(f"üí° –ú–æ–¥–µ–ª—å {self.model} —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ –¥–ª—è Steam Deck")
                print(f"üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å: ollama pull llama3.2:1b")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    async def process_command(self, user_command: str, screenshot: Optional[Image.Image] = None) -> Optional[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞
        
        Args:
            user_command: –ö–æ–º–∞–Ω–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ
            screenshot: –°–∫—Ä–∏–Ω—à–æ—Ç —ç–∫—Ä–∞–Ω–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –∏ —Ç–æ—á–Ω—ã–º–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä—è–º–æ–π –∞–Ω–∞–ª–∏–∑ LLM (–±–µ–∑ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –∑–¥–µ—Å—å)
            # –ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –∏–∑ bot.py —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–º
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            context_prompt = self._build_command_prompt(user_command, screenshot)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM
            response = await self._query_llm(context_prompt, screenshot)
            
            if not response:
                return None
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            result = self._parse_llm_response(response)
            if result:
                result['method'] = 'llm_fallback'
            
            return result
            
        except Exception as e:
            print(f"Error processing command: {e}")
            return None
    
    async def analyze_for_elements(self, screenshot: Image.Image, command: str) -> Optional[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª–∏–∑ —ç–∫—Ä–∞–Ω–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω—ã–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–º)
        
        Args:
            screenshot: –°–∫—Ä–∏–Ω—à–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            command: –ö–æ–º–∞–Ω–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏ –ø–æ–∏—Å–∫–æ–≤—ã–º–∏ —Ü–µ–ª—è–º–∏
        """
        try:
            width, height = screenshot.size
            
            analysis_prompt = f"""
–ö–æ–º–∞–Ω–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{command}"

–ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–∫—Ä–∏–Ω—à–æ—Ç –∏–≥—Ä—ã Disco Elysium ({width}x{height} –ø–∏–∫—Å–µ–ª–µ–π) –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ —á—Ç–æ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã.

–ù–ï –ì–ï–ù–ï–†–ò–†–£–ô –î–ï–ô–°–¢–í–ò–Ø! –¢–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–∏ —á—Ç–æ –∏—Å–∫–∞—Ç—å –Ω–∞ —ç–∫—Ä–∞–Ω–µ.

–í–µ—Ä–Ω–∏ JSON —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –ø–æ–ª—è–º–∏:
{{
    "analysis": "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —á—Ç–æ –≤–∏–¥–Ω–æ –Ω–∞ —ç–∫—Ä–∞–Ω–µ",
    "search_targets": [
        {{
            "text": "—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞ —ç–∫—Ä–∞–Ω–µ",
            "type": "button|text|dialogue|menu",
            "description": "–æ–ø–∏—Å–∞–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞"
        }}
    ],
    "reasoning": "–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ª–æ–≥–∏–∫–∏ –ø–æ–∏—Å–∫–∞"
}}

–ü–†–ò–ú–ï–†–´ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ü–µ–ª–µ–π:
- –î–ª—è "–Ω–æ–≤–∞—è –∏–≥—Ä–∞" ‚Üí {{"text": "–ù–æ–≤–∞—è –∏–≥—Ä–∞", "type": "button"}}
- –î–ª—è "–ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å" ‚Üí {{"text": "–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å", "type": "button"}}  
- –î–ª—è –≤—ã–±–æ—Ä–∞ –¥–∏–∞–ª–æ–≥–∞ ‚Üí {{"text": "—Ç–µ–∫—Å—Ç –≤–∞—Ä–∏–∞–Ω—Ç–∞", "type": "dialogue"}}
"""
            
            response = await self._query_vision_llm(analysis_prompt, screenshot)
            
            if not response:
                return None
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç (–æ–∂–∏–¥–∞–µ–º JSON)
            response_text = response.get('response', '') if 'response' in response else str(response)
            
            try:
                result = json.loads(response_text)
                result['success'] = True
                return result
            except (json.JSONDecodeError, ValueError):
                print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç LLM: {response_text}")
                return {
                    'analysis': 'JSON parsing failed',
                    'search_targets': [],
                    'reasoning': f'LLM response: {response_text}',
                    'success': False
                }
                
        except Exception as e:
            print(f"Error analyzing for elements: {e}")
            return None

    async def describe_screen(self, screenshot: Image.Image) -> Optional[str]:
        """
        –û–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —ç–∫—Ä–∞–Ω–∞
        
        Args:
            screenshot: –°–∫—Ä–∏–Ω—à–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —ç–∫—Ä–∞–Ω–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            prompt = self.config.vision.describe_prompt
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ vision –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            response = await self._query_vision_llm(prompt, screenshot)
            
            if not response:
                print("‚ùå Vision –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∏–ª–∏ –Ω–µ –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
                return None
            
            # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            if response:
                print(f"üîç Vision LLM response keys: {list(response.keys())}")
                print(f"üîç Vision LLM response: {response}")
            
            # Ollama API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É: {"response": "—Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞", ...}
            if response and 'response' in response:
                return response['response']
            elif response and 'message' in response:
                return response['message']['content']
            
            print("‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç Vision LLM")
            return None
            
        except Exception as e:
            print(f"Error describing screen: {e}")
            return None
    
    def _build_command_prompt(self, user_command: str, screenshot: Optional[Image.Image] = None) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–∞–Ω–¥—ã"""
        base_prompt = self.config.llm.system_prompt
        
        context = ""
        if screenshot:
            width, height = screenshot.size
            context = f"\n\n–ù–∞ —ç–∫—Ä–∞–Ω–µ —Å–µ–π—á–∞—Å –≤–∏–¥–Ω–æ –∏–≥—Ä–æ–≤–æ–µ –æ–∫–Ω–æ Disco Elysium —Ä–∞–∑–º–µ—Ä–æ–º {width}x{height} –ø–∏–∫—Å–µ–ª–µ–π. –í–ù–ò–ú–ê–¢–ï–õ–¨–ù–û –∏–∑—É—á–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –Ω–∞–π–¥–∏ –Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞."
        
        user_prompt = f"""
–ö–æ–º–∞–Ω–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{user_command}"
{context}

–í–ù–ò–ú–ê–¢–ï–õ–¨–ù–û –∏–∑—É—á–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —ç–∫—Ä–∞–Ω–∞ –∏–≥—Ä—ã Disco Elysium.

–ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –û–ü–†–ï–î–ï–õ–ï–ù–ò–Æ –ö–û–û–†–î–ò–ù–ê–¢:
1. –ù–∞–π–¥–∏ –Ω—É–∂–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (–∫–Ω–æ–ø–∫—É, —Ç–µ–∫—Å—Ç, –æ–±—ä–µ–∫—Ç)
2. –û–ø—Ä–µ–¥–µ–ª–∏ –µ–≥–æ —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
3. –í—ã—á–∏—Å–ª–∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä —ç–ª–µ–º–µ–Ω—Ç–∞
4. –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –∫–ª–∏–∫–∞

–ü–†–ò–ú–ï–†–´ –ê–ù–ê–õ–ò–ó–ê:
- –ï—Å–ª–∏ –∏—â–µ—à—å –∫–Ω–æ–ø–∫—É "–ù–æ–≤–∞—è –∏–≥—Ä–∞" - –æ–Ω–∞ –æ–±—ã—á–Ω–æ –≤ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–∏ —ç–∫—Ä–∞–Ω–∞
- –ï—Å–ª–∏ –∏—â–µ—à—å –≤–∞—Ä–∏–∞–Ω—Ç –¥–∏–∞–ª–æ–≥–∞ - –æ–Ω–∏ —Å–ª–µ–≤–∞, –≤ –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–º —Å–ø–∏—Å–∫–µ
- –ï—Å–ª–∏ –∏—â–µ—à—å —ç–ª–µ–º–µ–Ω—Ç –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è - –æ–ø—Ä–µ–¥–µ–ª–∏ –µ–≥–æ –ø–æ–∑–∏—Ü–∏—é –≤ —Å–µ—Ç–∫–µ

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–º–∞–Ω–¥—É –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–≥—Ä–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è —Å –¢–û–ß–ù–´–ú–ò –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏.
–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        """
        
        return base_prompt + user_prompt
    
    async def _query_llm(self, prompt: str, screenshot: Optional[Image.Image] = None) -> Optional[Dict[str, Any]]:
        """–ó–∞–ø—Ä–æ—Å –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–π LLM"""
        provider = self.config.llm.provider.lower()
        
        if provider == "openai" or provider == "deepseek" or provider == "anthropic":
            return await self._query_openai_api(prompt)
        else:
            return await self._query_ollama_api(prompt)
    
    async def _query_ollama_api(self, prompt: str) -> Optional[Dict[str, Any]]:
        """–ó–∞–ø—Ä–æ—Å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π Ollama"""
        try:
            session = await self._get_session()
            
            print(f"ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama –º–æ–¥–µ–ª–∏: {self.model}")
            
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.config.llm.temperature,
                    "num_predict": self.config.llm.max_tokens
                }
            }
            
            async with session.post(f"{self.base_url}/api/generate", json=payload) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    error_text = await response.text()
                    print(f"Ollama API error {response.status}: {error_text}")
                    
        except Exception as e:
            error_type = type(e).__name__
            if "ClientConnectorError" in error_type:
                print(f"‚ùå –ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É ({self.base_url})")
                print(f"üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: systemctl --user status ollama")
            elif "Timeout" in error_type:
                print(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama: {e}")
                print(f"üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–Ω–µ—à–Ω–∏–π API: provider: 'openai' –∏–ª–∏ 'deepseek'")
            else:
                print(f"‚ùå Error querying Ollama: {e}")
                import traceback
                traceback.print_exc()
        
        return None
    
    async def _query_openai_api(self, prompt: str) -> Optional[Dict[str, Any]]:
        """–ó–∞–ø—Ä–æ—Å –∫ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º—É API (OpenAI, DeepSeek, etc.)"""
        try:
            session = await self._get_session()
            
            print(f"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ {self.config.llm.provider} –º–æ–¥–µ–ª–∏: {self.model}")
            
            # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á –∏ URL
            api_key = getattr(self.config.llm, 'api_key', None)
            if not api_key:
                print(f"‚ùå API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è {self.config.llm.provider}")
                return None
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º payload –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": self.config.llm.temperature,
                "max_tokens": self.config.llm.max_tokens
            }
            
            async with session.post(f"{self.base_url}/v1/chat/completions", 
                                  json=payload, headers=headers) as response:
                if response.status == 200:
                    result = await response.json()
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç Ollama –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    if 'choices' in result and len(result['choices']) > 0:
                        content = result['choices'][0]['message']['content']
                        return {"response": content}
                    return None
                else:
                    error_text = await response.text()
                    print(f"{self.config.llm.provider} API error {response.status}: {error_text}")
                    
        except Exception as e:
            print(f"‚ùå Error querying {self.config.llm.provider} API: {e}")
            import traceback
            traceback.print_exc()
        
        return None
    
    async def _query_vision_llm(self, prompt: str, screenshot: Image.Image) -> Optional[Dict[str, Any]]:
        """–ó–∞–ø—Ä–æ—Å –∫ vision LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        provider = self.config.llm.provider.lower()
        
        if provider == "openai":
            return await self._query_openai_vision_api(prompt, screenshot)
        else:
            return await self._query_ollama_vision_api(prompt, screenshot)
    
    async def _query_ollama_vision_api(self, prompt: str, screenshot: Image.Image) -> Optional[Dict[str, Any]]:
        """–ó–∞–ø—Ä–æ—Å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π Ollama vision –º–æ–¥–µ–ª–∏"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64
            print(f"üñºÔ∏è  –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {screenshot.size} –≤ base64...")
            img_buffer = io.BytesIO()
            screenshot.save(img_buffer, format='PNG')
            img_data = img_buffer.getvalue()
            img_base64 = base64.b64encode(img_data).decode('utf-8')
            
            print(f"üìè –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {len(img_data)} –±–∞–π—Ç, base64: {len(img_base64)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            session = await self._get_session()
            
            payload = {
                "model": self.vision_model,
                "prompt": prompt,
                "images": [img_base64],
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 500
                }
            }
            
            async with session.post(f"{self.base_url}/api/generate", json=payload) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    error_text = await response.text()
                    print(f"Ollama Vision API error {response.status}: {error_text}")
                    
        except Exception as e:
            error_type = type(e).__name__
            if "Timeout" in error_type:
                print(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama vision –º–æ–¥–µ–ª–∏")
                print(f"üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–Ω–µ—à–Ω–∏–π API: provider: 'openai'")
            else:
                print(f"Error querying Ollama vision: {e}")
                import traceback
                traceback.print_exc()
        
        return None
    
    async def _query_openai_vision_api(self, prompt: str, screenshot: Image.Image) -> Optional[Dict[str, Any]]:
        """–ó–∞–ø—Ä–æ—Å –∫ OpenAI Vision API"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64
            print(f"üñºÔ∏è  –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {screenshot.size} –≤ base64...")
            
            img_buffer = io.BytesIO()
            screenshot.save(img_buffer, format='PNG')  # PNG –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç RGBA –∏ –ª—É—á—à–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
            img_data = img_buffer.getvalue()
            img_base64 = base64.b64encode(img_data).decode('utf-8')
            
            print(f"üìè –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {len(img_data)} –±–∞–π—Ç")
            
            session = await self._get_session()
            
            api_key = getattr(self.config.llm, 'api_key', None)
            if not api_key:
                print("‚ùå API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è OpenAI")
                return None
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º payload –¥–ª—è vision API
            payload = {
                "model": self.vision_model,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{img_base64}"
                                }
                            }
                        ]
                    }
                ],
                "temperature": 0.1,
                "max_tokens": 500
            }
            
            print(f"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenAI Vision API...")
            
            async with session.post(f"{self.base_url}/v1/chat/completions", 
                                  json=payload, headers=headers) as response:
                if response.status == 200:
                    result = await response.json()
                    if 'choices' in result and len(result['choices']) > 0:
                        content = result['choices'][0]['message']['content']
                        return {"response": content}
                    return None
                else:
                    error_text = await response.text()
                    print(f"OpenAI Vision API error {response.status}: {error_text}")
                    
        except Exception as e:
            print(f"‚ùå Error querying OpenAI Vision API: {e}")
            import traceback
            traceback.print_exc()
        
        return None
    
    def _parse_llm_response(self, response: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            print(f"üîç –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç LLM: {response}")
            
            if 'response' not in response:
                print("‚ùå –ö–ª—é—á 'response' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ LLM")
                return None
            
            response_text = response['response'].strip()
            print(f"üîç –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞: {response_text[:200]}...")
            
            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
            if response_text.startswith('{') and response_text.endswith('}'):
                return json.loads(response_text)
            
            # –ò—â–µ–º JSON –≤ —Ç–µ–∫—Å—Ç–µ
            json_start = response_text.find('{')
            json_end = response_text.rfind('}')
            
            if json_start >= 0 and json_end > json_start:
                json_str = response_text[json_start:json_end + 1]
                return json.loads(json_str)
            
            # –ï—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            return {
                "actions": [],
                "description": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–Ω—è—Ç—å –∫–æ–º–∞–Ω–¥—É"
            }
            
        except json.JSONDecodeError:
            print(f"Failed to parse JSON from LLM response: {response.get('response', '')}")
            return None
        except Exception as e:
            print(f"Error parsing LLM response: {e}")
            return None
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ HTTP —Å–µ—Å—Å–∏–∏"""
        if self.session and not self.session.closed:
            await self.session.close()
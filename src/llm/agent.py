"""
LLM Agent –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–∞–Ω–¥ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å Ollama
"""
import json
import asyncio
import aiohttp
from typing import Dict, List, Optional, Any
from PIL import Image
import io
import base64

from ..utils.config import Config


class LLMAgent:
    """–ê–≥–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ Ollama"""
    
    def __init__(self, config: Config):
        self.config = config
        self.base_url = config.llm.base_url
        self.model = config.llm.model
        self.vision_model = config.llm.vision_model
        self.session = None
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ HTTP —Å–µ—Å—Å–∏–∏"""
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=180)  # 3 –º–∏–Ω—É—Ç—ã –¥–ª—è Steam Deck —Å —É—á–µ—Ç–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            )
        return self.session
    
    async def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama"""
        try:
            session = await self._get_session()
            async with session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
                    models_data = await response.json()
                    available_models = [model['name'] for model in models_data.get('models', [])]
                    print(f"ü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")
                    
                    if self.model not in available_models:
                        print(f"‚ùå –ú–æ–¥–µ–ª—å {self.model} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
                        print(f"üí° –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å: ollama pull {self.model}")
                        return False
                    
                    return True
                return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Ollama: {e}")
            return False
    
    async def test_model(self) -> bool:
        """–ü—Ä–æ—Å—Ç–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∫–æ—Ä–æ—Ç–∫–∏–º –∑–∞–ø—Ä–æ—Å–æ–º"""
        try:
            print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å {self.model} –ø—Ä–æ—Å—Ç—ã–º –∑–∞–ø—Ä–æ—Å–æ–º...")
            
            session = await self._get_session()
            test_payload = {
                "model": self.model,
                "prompt": "–ü—Ä–∏–≤–µ—Ç! –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: —Ä–∞–±–æ—Ç–∞–µ—Ç?",
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 10  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
                }
            }
            
            import time
            start_time = time.time()
            
            async with session.post(f"{self.base_url}/api/generate", json=test_payload) as response:
                elapsed = time.time() - start_time
                
                if response.status == 200:
                    result = await response.json()
                    response_text = result.get('response', '').strip()
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ {elapsed:.1f}—Å: '{response_text}'")
                    return True
                else:
                    error_text = await response.text()
                    print(f"‚ùå –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ –Ω–µ—É–¥–∞—á–µ–Ω {response.status}: {error_text}")
                    return False
                    
        except Exception as e:
            error_type = type(e).__name__
            if "Timeout" in error_type:
                print(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ (>180s)")
                print(f"üí° –ú–æ–¥–µ–ª—å {self.model} —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ –¥–ª—è Steam Deck")
                print(f"üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å: ollama pull llama3.2:1b")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    async def process_command(self, user_command: str, screenshot: Optional[Image.Image] = None) -> Optional[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–≥—Ä–æ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
        
        Args:
            user_command: –ö–æ–º–∞–Ω–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ
            screenshot: –°–∫—Ä–∏–Ω—à–æ—Ç —ç–∫—Ä–∞–Ω–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            context_prompt = self._build_command_prompt(user_command, screenshot)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM
            response = await self._query_llm(context_prompt, screenshot)
            
            if not response:
                return None
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            return self._parse_llm_response(response)
            
        except Exception as e:
            print(f"Error processing command: {e}")
            return None
    
    async def describe_screen(self, screenshot: Image.Image) -> Optional[str]:
        """
        –û–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —ç–∫—Ä–∞–Ω–∞
        
        Args:
            screenshot: –°–∫—Ä–∏–Ω—à–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —ç–∫—Ä–∞–Ω–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            prompt = self.config.vision.describe_prompt
            
            response = await self._query_vision_llm(prompt, screenshot)
            
            # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            if response:
                print(f"üîç LLM response keys: {list(response.keys())}")
                print(f"üîç LLM response: {response}")
            
            # Ollama API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É: {"response": "—Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞", ...}
            if response and 'response' in response:
                return response['response']
            elif response and 'message' in response:
                return response['message']['content']
            
            print("‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ LLM")
            return None
            
        except Exception as e:
            print(f"Error describing screen: {e}")
            return None
    
    def _build_command_prompt(self, user_command: str, screenshot: Optional[Image.Image] = None) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–∞–Ω–¥—ã"""
        base_prompt = self.config.llm.system_prompt
        
        context = ""
        if screenshot:
            context = "\n\n–ù–∞ —ç–∫—Ä–∞–Ω–µ —Å–µ–π—á–∞—Å –≤–∏–¥–Ω–æ –∏–≥—Ä–æ–≤–æ–µ –æ–∫–Ω–æ Disco Elysium. –£—á—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π."
        
        user_prompt = f"""
–ö–æ–º–∞–Ω–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{user_command}"
{context}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–º–∞–Ω–¥—É –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–≥—Ä–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è.
–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        """
        
        return base_prompt + user_prompt
    
    async def _query_llm(self, prompt: str, screenshot: Optional[Image.Image] = None) -> Optional[Dict[str, Any]]:
        """–ó–∞–ø—Ä–æ—Å –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–π LLM"""
        try:
            session = await self._get_session()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            print(f"ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏: {self.model}")
            
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.config.llm.temperature,
                    "num_predict": self.config.llm.max_tokens
                }
            }
            
            async with session.post(f"{self.base_url}/api/generate", json=payload) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    error_text = await response.text()
                    print(f"LLM API error {response.status}: {error_text}")
                    
        except Exception as e:
            error_type = type(e).__name__
            if "ClientConnectorError" in error_type:
                print(f"‚ùå –ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É ({self.base_url})")
                print(f"üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: systemctl --user status ollama")
            elif "Timeout" in error_type:
                print(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ LLM: {e}")
                print(f"üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å —Ç–∞–π–º–∞—É—Ç –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª—å {self.model}")
            elif "JSONDecodeError" in error_type:
                print(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –æ—Ç–≤–µ—Ç–∞: {e}")
            else:
                print(f"‚ùå Error querying LLM: {e}")
                print(f"üîç URL: {self.base_url}/api/generate")
                print(f"üîç Model: {self.model}")
                import traceback
                traceback.print_exc()
        
        return None
    
    async def _query_vision_llm(self, prompt: str, screenshot: Image.Image) -> Optional[Dict[str, Any]]:
        """–ó–∞–ø—Ä–æ—Å –∫ vision LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64
            print(f"üñºÔ∏è  –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {screenshot.size} –≤ base64...")
            img_buffer = io.BytesIO()
            screenshot.save(img_buffer, format='PNG')
            img_data = img_buffer.getvalue()
            img_base64 = base64.b64encode(img_data).decode('utf-8')
            
            print(f"üìè –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {len(img_data)} –±–∞–π—Ç, base64: {len(img_base64)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            session = await self._get_session()
            
            payload = {
                "model": self.vision_model,
                "prompt": prompt,
                "images": [img_base64],
                "stream": False,
                "options": {
                    "temperature": 0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è
                    "num_predict": 500
                }
            }
            
            async with session.post(f"{self.base_url}/api/generate", json=payload) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    error_text = await response.text()
                    print(f"Vision LLM API error {response.status}: {error_text}")
                    
        except Exception as e:
            error_type = type(e).__name__
            if "ClientConnectorError" in error_type:
                print(f"‚ùå –ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É –¥–ª—è vision –º–æ–¥–µ–ª–∏")
                print(f"üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –º–æ–¥–µ–ª—å {self.vision_model} –∑–∞–≥—Ä—É–∂–µ–Ω–∞: ollama pull {self.vision_model}")
            else:
                print(f"Error querying vision LLM: {e}")
                import traceback
                traceback.print_exc()
        
        return None
    
    def _parse_llm_response(self, response: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            print(f"üîç –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç LLM: {response}")
            
            if 'response' not in response:
                print("‚ùå –ö–ª—é—á 'response' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ LLM")
                return None
            
            response_text = response['response'].strip()
            print(f"üîç –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞: {response_text[:200]}...")
            
            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
            if response_text.startswith('{') and response_text.endswith('}'):
                return json.loads(response_text)
            
            # –ò—â–µ–º JSON –≤ —Ç–µ–∫—Å—Ç–µ
            json_start = response_text.find('{')
            json_end = response_text.rfind('}')
            
            if json_start >= 0 and json_end > json_start:
                json_str = response_text[json_start:json_end + 1]
                return json.loads(json_str)
            
            # –ï—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            return {
                "actions": [],
                "description": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–Ω—è—Ç—å –∫–æ–º–∞–Ω–¥—É"
            }
            
        except json.JSONDecodeError:
            print(f"Failed to parse JSON from LLM response: {response.get('response', '')}")
            return None
        except Exception as e:
            print(f"Error parsing LLM response: {e}")
            return None
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ HTTP —Å–µ—Å—Å–∏–∏"""
        if self.session and not self.session.closed:
            await self.session.close()